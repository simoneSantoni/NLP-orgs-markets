{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 ― main topics\n",
    "\n",
    "+ representing the duality between words and meanings\n",
    "+ language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models of natural language (NL)\n",
    "\n",
    "**What** is an NL model?\n",
    "\n",
    "**How** do we build an NL model?\n",
    "\n",
    "**Why** should we care about NL models?\n",
    "\n",
    "... let's focus on the **why** aspect first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care about NL models?\n",
    "\n",
    "Let's consider **tokenization**, a core task to any natural language processing\n",
    "analysis.\n",
    "\n",
    "Now, let's apply different tokenizers to the below displayed sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"Back in the golden age of hip-hop\n",
    "       (the late '80s, youngsters), Rakim took\n",
    "       lyricism to unfathomable heights,\n",
    "       helping to usher in the wave of lethal\n",
    "       MCs like Big Daddy Kane and Kool G Rap,\n",
    "       who would go on to become icons. Two\n",
    "       decades later, some of Ra's rhymes from\n",
    "       '86 are still over people's heads: His\n",
    "       wordplay remains a hip-hop measuring\n",
    "       stick.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers in action: Naive solution\n",
    "\n",
    "Both the module `re` and string methods can be used to implement a naive\n",
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "================\n",
      "Back in the golden age of hip-hop\n",
      "       (the late '80s, youngsters), Rakim took\n",
      "       lyricism to unfathomable heights,\n",
      "       helping to usher in the wave of lethal\n",
      "       MCs like Big Daddy Kane and Kool G Rap,\n",
      "       who would go on to become icons. Two\n",
      "       decades later, some of Ra's rhymes from\n",
      "       '86 are still over people's heads: His\n",
      "       wordplay remains a hip-hop measuring\n",
      "       stick.\n",
      "\n",
      "Pure Python:\n",
      "============\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip-hop', '(the', 'late', \"'80s,\", 'youngsters),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights,', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap,', 'who', 'would', 'go', 'on', 'to', 'become', 'icons.', 'Two', 'decades', 'later,', 'some', 'of', \"Ra's\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', \"people's\", 'heads:', 'His', 'wordplay', 'remains', 'a', 'hip-hop', 'measuring', 'stick.']\n",
      "\n",
      "With regex:\n",
      "===========\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', 'hop', '(the', 'late', \"'80s\", 'youngsters)', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', 'Two', 'decades', 'later', 'some', 'of', \"Ra's\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', \"people's\", 'heads:', 'His', 'wordplay', 'remains', 'a', 'hip', 'hop', 'measuring', 'stick', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pure Python\n",
    "tokens_s = s.split()\n",
    "\n",
    "# regex\n",
    "import re\n",
    "pattern = r'[-\\s.,;!?]+'\n",
    "tokens_re = re.split(pattern, s)\n",
    "\n",
    "# print results\n",
    "from pprint import pprint as pp\n",
    "print(\"\"\"\n",
    "Original string:\n",
    "================\n",
    "{}\n",
    "\n",
    "Pure Python:\n",
    "============\n",
    "{}\n",
    "\n",
    "With regex:\n",
    "===========\n",
    "{}\n",
    "\"\"\".format(s, tokens_s, tokens_re), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers in action: NLTK's tokenizers\n",
    "\n",
    "NLTK has different tokenizers (which, in 2020, seem gross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "================\n",
      "Back in the golden age of hip-hop\n",
      "       (the late '80s, youngsters), Rakim took\n",
      "       lyricism to unfathomable heights,\n",
      "       helping to usher in the wave of lethal\n",
      "       MCs like Big Daddy Kane and Kool G Rap,\n",
      "       who would go on to become icons. Two\n",
      "       decades later, some of Ra's rhymes from\n",
      "       '86 are still over people's heads: His\n",
      "       wordplay remains a hip-hop measuring\n",
      "       stick.\n",
      "\n",
      "Pure Python:\n",
      "============\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip-hop', '(the', 'late', \"'80s,\", 'youngsters),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights,', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap,', 'who', 'would', 'go', 'on', 'to', 'become', 'icons.', 'Two', 'decades', 'later,', 'some', 'of', \"Ra's\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', \"people's\", 'heads:', 'His', 'wordplay', 'remains', 'a', 'hip-hop', 'measuring', 'stick.']\n",
      "\n",
      "With regex:\n",
      "===========\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', 'hop', '(the', 'late', \"'80s\", 'youngsters)', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', 'Two', 'decades', 'later', 'some', 'of', \"Ra's\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', \"people's\", 'heads:', 'His', 'wordplay', 'remains', 'a', 'hip', 'hop', 'measuring', 'stick', '']\n",
      "\n",
      "With NLTK's Regex tokenizer:\n",
      "============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', '-hop', '(the', 'late', \"'80s,\", 'youngsters', '),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', '.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip', '-hop', 'measuring', 'stick', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import NLTK's tokenizer 'Regexp'\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# the tokenizer\n",
    "pattern = r'\\w+|$[0-9.]+|\\S+'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "# tokenize\n",
    "tokens_nltk_r = tokenizer.tokenize(s)\n",
    "\n",
    "# print results\n",
    "from pprint import pprint as pp\n",
    "print(\"\"\"\n",
    "Original string:\n",
    "================\n",
    "{}\n",
    "\n",
    "Pure Python:\n",
    "============\n",
    "{}\n",
    "\n",
    "With regex:\n",
    "===========\n",
    "{}\n",
    "\n",
    "With NLTK's Regex tokenizer:\n",
    "============================\n",
    "{}\n",
    "\"\"\".format(s, tokens_s, tokens_re, tokens_nltk_r), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "================\n",
      "Back in the golden age of hip-hop\n",
      "       (the late '80s, youngsters), Rakim took\n",
      "       lyricism to unfathomable heights,\n",
      "       helping to usher in the wave of lethal\n",
      "       MCs like Big Daddy Kane and Kool G Rap,\n",
      "       who would go on to become icons. Two\n",
      "       decades later, some of Ra's rhymes from\n",
      "       '86 are still over people's heads: His\n",
      "       wordplay remains a hip-hop measuring\n",
      "       stick.\n",
      "\n",
      "With NLTK's Regex tokenizer:\n",
      "============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', '-hop', '(the', 'late', \"'80s,\", 'youngsters', '),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', '.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip', '-hop', 'measuring', 'stick', '.']\n",
      "\n",
      "With NLTK's Treebank tokenizer:\n",
      "===============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip-hop', '(', 'the', 'late', \"'80s\", ',', 'youngsters', ')', ',', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip-hop', 'measuring', 'stick', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import NLTK's Treebank tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# the tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# tokenize\n",
    "tokens_nltk_t = tokenizer.tokenize(s)\n",
    "\n",
    "# print results\n",
    "from pprint import pprint as pp\n",
    "print(\"\"\"\n",
    "Original string:\n",
    "================\n",
    "{}\n",
    "\n",
    "With NLTK's Regex tokenizer:\n",
    "============================\n",
    "{}\n",
    "\n",
    "With NLTK's Treebank tokenizer:\n",
    "===============================\n",
    "{}\n",
    "\"\"\".format(s, tokens_nltk_r, tokens_nltk_t), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers in action: spaCy's tokenizer\n",
    "\n",
    "spaCy operates a tokenizer that is informed by regular expressions & **DL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy\n",
    "import spacy\n",
    "\n",
    "# load a model of natural language (https://spacy.io/models)\n",
    "'''\n",
    "to install one of spaCy's models:\n",
    "\n",
    "conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "'''\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With NLTK's Regex tokenizer:\n",
      "============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', '-hop', '(the', 'late', \"'80s,\", 'youngsters', '),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', '.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip', '-hop', 'measuring', 'stick', '.']\n",
      "\n",
      "With NLTK's Treebank tokenizer:\n",
      "===============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip-hop', '(', 'the', 'late', \"'80s\", ',', 'youngsters', ')', ',', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip-hop', 'measuring', 'stick', '.']\n",
      "\n",
      "With spaCy:\n",
      "==========\n",
      "Back ^^ in ^^ the ^^ golden ^^ age ^^ of ^^ hip ^^ - ^^ hop ^^ ( ^^ the ^^ late ^^ ' ^^ 80s ^^ , ^^ youngsters ^^ ) ^^ , ^^ Rakim ^^ took ^^ lyricism ^^ to ^^ unfathomable ^^ heights ^^ , ^^ helping ^^ to ^^ usher ^^ in ^^ the ^^ wave ^^ of ^^ lethal ^^ MCs ^^ like ^^ Big ^^ Daddy ^^ Kane ^^ and ^^ Kool ^^ G ^^ Rap ^^ , ^^ who ^^ would ^^ go ^^ on ^^ to ^^ become ^^ icons ^^ . ^^ Two ^^ decades ^^ later ^^ , ^^ some ^^ of ^^ Ra ^^ 's ^^ rhymes ^^ from ^^ ' ^^ 86 ^^ are ^^ still ^^ over ^^ people ^^ 's ^^ heads ^^ : ^^ His ^^ wordplay ^^ remains ^^ a ^^ hip ^^ - ^^ hop ^^ measuring ^^ stick ^^ .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the nlp pipeline\n",
    "nlp = en_core_web_sm.load()\n",
    "'''\n",
    "Mac users may want to go for:\n",
    "\n",
    "spacy.load(\"the_model\")\n",
    "'''\n",
    "\n",
    "# pass the sentence through the pipeline\n",
    "doc = nlp(s)\n",
    "\n",
    "# store tokens in a list\n",
    "tokens_spacy = [token.text for token in doc if '\\n' not in token.text]\n",
    "\n",
    "# print\n",
    "print(\"\"\"\n",
    "With NLTK's Regex tokenizer:\n",
    "============================\n",
    "{}\n",
    "\n",
    "With NLTK's Treebank tokenizer:\n",
    "===============================\n",
    "{}\n",
    "\n",
    "With spaCy:\n",
    "==========\n",
    "{}\n",
    "\n",
    "\"\"\".format(tokens_nltk_r, tokens_nltk_t, ' ^^ '.join(tokens_spacy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudocode behind spaCy's tokenizer**\n",
    "\n",
    "![](images/_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy's tokenizer in context**\n",
    "\n",
    "![](images/_17.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With NLTK's Regex tokenizer:\n",
      "============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip', '-hop', '(the', 'late', \"'80s,\", 'youngsters', '),', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons', '.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip', '-hop', 'measuring', 'stick', '.']\n",
      "\n",
      "With NLTK's Treebank tokenizer:\n",
      "===============================\n",
      "['Back', 'in', 'the', 'golden', 'age', 'of', 'hip-hop', '(', 'the', 'late', \"'80s\", ',', 'youngsters', ')', ',', 'Rakim', 'took', 'lyricism', 'to', 'unfathomable', 'heights', ',', 'helping', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'MCs', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icons.', 'Two', 'decades', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhymes', 'from', \"'86\", 'are', 'still', 'over', 'people', \"'s\", 'heads', ':', 'His', 'wordplay', 'remains', 'a', 'hip-hop', 'measuring', 'stick', '.']\n",
      "\n",
      "With spaCy -- tokens as text:\n",
      "=============================\n",
      "Back ^^ in ^^ the ^^ golden ^^ age ^^ of ^^ hip ^^ - ^^ hop ^^ ( ^^ the ^^ late ^^ ' ^^ 80s ^^ , ^^ youngsters ^^ ) ^^ , ^^ Rakim ^^ took ^^ lyricism ^^ to ^^ unfathomable ^^ heights ^^ , ^^ helping ^^ to ^^ usher ^^ in ^^ the ^^ wave ^^ of ^^ lethal ^^ MCs ^^ like ^^ Big ^^ Daddy ^^ Kane ^^ and ^^ Kool ^^ G ^^ Rap ^^ , ^^ who ^^ would ^^ go ^^ on ^^ to ^^ become ^^ icons ^^ . ^^ Two ^^ decades ^^ later ^^ , ^^ some ^^ of ^^ Ra ^^ 's ^^ rhymes ^^ from ^^ ' ^^ 86 ^^ are ^^ still ^^ over ^^ people ^^ 's ^^ heads ^^ : ^^ His ^^ wordplay ^^ remains ^^ a ^^ hip ^^ - ^^ hop ^^ measuring ^^ stick ^^ .\n",
      "\n",
      "With spaCy -- tokens as lemmas:\n",
      "===============================\n",
      "['back', 'in', 'the', 'golden', 'age', 'of', 'hip', '-', 'hop', '(', 'the', 'late', \"'\", '80', ',', 'youngster', ')', ',', 'Rakim', 'take', 'lyricism', 'to', 'unfathomable', 'height', ',', 'help', 'to', 'usher', 'in', 'the', 'wave', 'of', 'lethal', 'mc', 'like', 'Big', 'Daddy', 'Kane', 'and', 'Kool', 'G', 'Rap', ',', 'who', 'would', 'go', 'on', 'to', 'become', 'icon', '.', 'two', 'decade', 'later', ',', 'some', 'of', 'Ra', \"'s\", 'rhyme', 'from', \"'\", '86', 'be', 'still', 'over', 'people', \"'s\", 'head', ':', '-PRON-', 'wordplay', 'remain', 'a', 'hip', '-', 'hop', 'measure', 'stick', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store tokens in a list\n",
    "lemmas_spacy = [token.lemma_ for token in doc if not '\\n' in token.text]\n",
    "\n",
    "# print\n",
    "print(\"\"\"\n",
    "With NLTK's Regex tokenizer:\n",
    "============================\n",
    "{}\n",
    "\n",
    "With NLTK's Treebank tokenizer:\n",
    "===============================\n",
    "{}\n",
    "\n",
    "With spaCy -- tokens as text:\n",
    "=============================\n",
    "{}\n",
    "\n",
    "With spaCy -- tokens as lemmas:\n",
    "===============================\n",
    "{}\n",
    "\n",
    "\"\"\".format(tokens_nltk_r, tokens_nltk_t, ' ^^ '.join(tokens_spacy),\n",
    "           lemmas_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a model of NL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "# example from spaCy's website\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How do we build a model of NL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliography**\n",
    "\n",
    "+ Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. Eliyahu Kiperwasser, Yoav Goldberg. (2016)\n",
    "\n",
    "+ A Dynamic Oracle for Arc-Eager Dependency Parsing. Yoav Goldberg, Joakim Nivre (2012)\n",
    "\n",
    "+ Parsing English in 500 Lines of Python. Matthew Honnibal (2013)\n",
    "\n",
    "+ Stack-propagation: Improved Representation Learning for Syntax. Yuan Zhang, David Weiss (2016)\n",
    "\n",
    "+ Deep multi-task learning with low level tasks supervised at lower layers. Anders Søgaard, Yoav Goldberg (2016)\n",
    "\n",
    "+ An Improved Non-monotonic Transition System for Dependency Parsing. Matthew Honnibal, Mark Johnson (2015)\n",
    "\n",
    "+ A Fast and Accurate Dependency Parser using Neural Networks. Danqi Cheng, Christopher D. Manning (2014)\n",
    "\n",
    "+ Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques. Stefan Riezler et al. (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tools**\n",
    "\n",
    "+ Old-school but still good:\n",
    "  + $\\texttt{word2vec} $\n",
    "+ Extensions of $\\texttt{\\word2vec}$:\n",
    "  + GloVe\n",
    "  + Fasttext\n",
    "+ Context-aware models:\n",
    "  + BERT\n",
    "  + ELMO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
